{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "0_6_optimization_tutorial_js.ipynb",
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiQpiJ4x4Dzi"
      },
      "source": [
        "「PyTorch入門  6. 最適化」\n",
        "===============================================================\n",
        "【原題】OPTIMIZING MODEL PARAMETERS\n",
        "\n",
        "【原著】\n",
        "[Suraj Subramanian](https://github.com/suraj813)、[Seth Juarez](https://github.com/sethjuarez/) 、[Cassie Breviu](https://github.com/cassieview/) 、[Dmitry Soshnikov](https://soshnikov.com/)、[Ari Bornstein](https://github.com/aribornstein/)\n",
        "\n",
        "\n",
        "【元URL】https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
        "\n",
        "【翻訳】電通国際情報サービスISID AIトランスフォーメーションセンター　小川 雄太郎\n",
        "\n",
        "【日付】2021年03月20日\n",
        "\n",
        "【チュトーリアル概要】\n",
        "\n",
        "本チュートリアルでは、オプティマイザー（Optimizer）を使用した、パラメータの最適化（≒学習）について解説を行います。\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPkyI-DJ38-R"
      },
      "source": [
        "パラメータの最適化\n",
        "===========================\n",
        "\n",
        "モデルとデータを用意できたので続いてはモデルを訓練、検証することで、データに対してモデルのパラメータを最適化し、テストを行います。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow4ZimTD48XR"
      },
      "source": [
        "モデルの訓練は反復的なプロセスとなります。\n",
        "\n",
        "各イテレーション（エポックと呼ばれます）で、モデルは出力を計算し、損失を求めます。そして各パラメータについて損失に対する偏微分の値を求めます。\n",
        "\n",
        "その後、勾配降下法に基づいてパラメータを最適化します。\n",
        "\n",
        "この最適化プロセスの流れについては、以下の動画も参考にご覧ください。\n",
        "\n",
        "- 動画：[backpropagation from 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz96ICKoEgie"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0HPbxJy480B"
      },
      "source": [
        "コード準備\n",
        "-----------------\n",
        "入門シリーズの「2. データセットとデータローダー」および、「4. モデル構築」からコードを再利用します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-QFvrJl38-L"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNpggd8W38-R"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PidFXE1LEjR1"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeozEcms38-S"
      },
      "source": [
        "ハイパーパラメータ\n",
        "-----------------\n",
        "\n",
        "ハイパーパラメータは、モデルの最適化プロセスを制御するためのパラメータです。\n",
        "\n",
        "ハイパーパラメータの値が異なると、モデルの学習や収束率に影響します（詳細なハイパーパラメータチューニングの解説は[こちら](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)をご覧ください）。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XWChnhIEtHH"
      },
      "source": [
        "\n",
        "今回は、訓練用のハイパーパラメータとして以下の値を使用します。\n",
        "\n",
        " - **Number of Epochs**：イテレーション回数\n",
        " - **Batch Size**：ミニバッチサイズを構成するデータ数\n",
        " - **Learning Rate**：パラメータ更新の係数。値が小さいと変化が少なく、大きすぎると訓練に失敗する可能性が生まれる\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpay3IJz38-U"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW8AYLUtExtf"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx1bOwA_38-V"
      },
      "source": [
        "最適化ループ\n",
        "-----------------\n",
        "ハイパーパラメータを設定後、訓練で最適化のループを回すことで、モデルを最適化します。\n",
        "\n",
        "最適化ループの1回のイテレーションは、**エポック(epoch)**と呼ばれます。\n",
        "\n",
        "各エポックでは2種類のループから構成されます。\n",
        "\n",
        " - **訓練ループ**：データセットに対して訓練を実行し、パラメータを収束させます\n",
        "\n",
        " - **検証 / テストループ**：テストデータセットでモデルを評価し、性能が向上しているか確認します\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8_uj3QLLxvT"
      },
      "source": [
        "訓練ループ内で使用される概念について、簡単に把握しておきましょう。\n",
        "\n",
        "本チュートリアルの最後には、最適化ループの完全な実装を紹介します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7JihOQJLx4B"
      },
      "source": [
        "**損失関数：Loss Function**\n",
        "\n",
        "データが与えられても、訓練されていないネットワークは正しい答えを出力しない可能性があります。\n",
        "\n",
        "損失関数はモデルが推論した結果と、実際の正解との誤差の大きさを測定する関数です。訓練ではこの損失関数の値を小さくしていきます。\n",
        "\n",
        "損失を計算するためには、入力データに対するモデルの推論結果を求め、その値と正解のラベルとの違いを比較します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbuOTl6wNm-7"
      },
      "source": [
        "一般的な損失関数としては、回帰タスクでは[`nn.MSELoss`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)(Mean Square Error)、分類タスクでは[`nn.NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)(Negative Log Likelihood) が使用されます。\n",
        "\n",
        "[`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)は、``nn.LogSoftmax`` と ``nn.NLLLoss``を結合した損失関数となります。\n",
        "\n",
        "モデルが出力する`logit`値を`nn.CrossEntropyLoss`に与えて正規化し、予測誤差を求めます。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習したこと\n",
        "- 回帰タスク: 具体的な数値を当てはめるタスク\n",
        "- 分類タスク: カテゴリ分類をするタスク\n",
        "- MSELoss: モデルが出力した予測と正解との距離を測る\n",
        "- NLLLoss: 正解であるラベルの予測確率だけを見て損失を計算する\n",
        "- LogSoftmax: モデルの生の出力（logits）を確率にする\n",
        "- CrossEntropyLoss$$L(w,b)\n",
        "= -\\sum_{i=1}^{n}\n",
        "\\Bigl[ y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\Bigr]$$"
      ],
      "metadata": {
        "id": "9aP-H4EQEsjk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayTh0SOU38-W"
      },
      "source": [
        "# loss functionの初期化、定義\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEUFRDx638-W"
      },
      "source": [
        "**最適化器：Optimizer**\n",
        "\n",
        "最適化は各訓練ステップにおいてモデルの誤差を小さくなるように、モデルパラメータを調整するプロセスです。\n",
        "\n",
        "<br>\n",
        "\n",
        "**最適化アルゴリズム：Optimization algorithms**\n",
        "\n",
        "最適化アルゴリズムは、最適化プロセスの具体的な手続きです（本チュートリアルでは確率的勾配降下法：Stochastic Gradient Descentを使用します）。\n",
        "\n",
        "最適化のロジックは全て``optimizer``オブジェクト内に隠ぺいされます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB8RLaw3PQ3Z"
      },
      "source": [
        " 今回はSGD optimizerを使用します。ただし、最適化関数にはADAMやRMSPropなど、様々な種類があります。\n",
        "\n",
        " 詳細については、[こちら](https://pytorch.org/docs/stable/optim.html)を参照ください。\n",
        " ## 学習したこと\n",
        "RMSPropとAdamについてはもともと勉強していたのでnotionにまとめていた。どちらもパラメータ更新の振動を抑えるという目的で導入されたもの。1度の更新が大きすぎると、振動を起こしてしまうため最小値にたどり着くのが遅い。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMwa7PPmFzjv"
      },
      "source": [
        "\n",
        "訓練したいモデルパラメータをoptimizerに登録し、合わせて学習率をハイパーパラメータとして渡すことで初期化を行います。\n",
        "\n",
        "\n",
        "## memo\n",
        "- optim.SGDは単純に求めた勾配を使用して学習率とかけて弾いてるだけ。確率的勾配降下法はDataLoaderのRandomがないと実現できない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIH9ERrX38-X"
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c53YLpx038-Y"
      },
      "source": [
        "訓練ループ内で、最適化（optimization）は3つのステップから構成されます。\n",
        "\n",
        "[1] ``optimizer.zero_grad()``を実行し、モデルパラメータの勾配をリセットします。\n",
        "\n",
        "勾配の計算は蓄積されていくので、毎イテレーション、明示的にリセットします。\n",
        "parameter.gradを削除する\n",
        "\n",
        "<br>\n",
        "\n",
        "[2] 続いて、``loss.backwards()``を実行し、バックプロパゲーションを実行します。\n",
        "\n",
        "PyTorchは損失に対する各パラメータの偏微分の値（勾配）を求めます。\n",
        "\n",
        "<br>\n",
        "\n",
        "[3] 最後に、``optimizer.step()``を実行し、各パラメータの勾配を使用してパラメータの値を調整します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4vSCUCEGBEz"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulov18zc38-Y"
      },
      "source": [
        "実装全体：Full Implementation\n",
        "-----------------------\n",
        "最適化を実行するコードをループする``train_loop``と、テストデータに対してモデルの性能を評価する``test_loop``を定義します。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVvdppwu38-Z"
      },
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # 予測と損失の計算\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # バックプロパゲーション\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    batch_total = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    # 累算代入演算子（結果はtest_loss, correctに格納される）\n",
        "    test_loss /= batch_total\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pq-tYFz38-Z"
      },
      "source": [
        "損失関数とoptimizerを初期化し、それを ``train_loop`` と ``test_loop`` に渡します。\n",
        "\n",
        "<br>\n",
        "\n",
        "以下の実装例において、モデルの性能を向上させるために、`epoch`数は自由に変えてみてください。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhpe8Lia38-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f03874ff-f9ba-4304-9a66-4346ea77851d"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.305504  [    0/60000]\n",
            "loss: 2.296744  [ 6400/60000]\n",
            "loss: 2.289486  [12800/60000]\n",
            "loss: 2.283658  [19200/60000]\n",
            "loss: 2.278168  [25600/60000]\n",
            "loss: 2.257974  [32000/60000]\n",
            "loss: 2.258087  [38400/60000]\n",
            "loss: 2.242993  [44800/60000]\n",
            "loss: 2.229284  [51200/60000]\n",
            "loss: 2.219522  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 47.9%, Avg loss: 2.220442 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.232752  [    0/60000]\n",
            "loss: 2.229776  [ 6400/60000]\n",
            "loss: 2.198536  [12800/60000]\n",
            "loss: 2.190271  [19200/60000]\n",
            "loss: 2.175537  [25600/60000]\n",
            "loss: 2.136775  [32000/60000]\n",
            "loss: 2.143347  [38400/60000]\n",
            "loss: 2.103628  [44800/60000]\n",
            "loss: 2.090970  [51200/60000]\n",
            "loss: 2.060287  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 56.4%, Avg loss: 2.059934 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.090377  [    0/60000]\n",
            "loss: 2.077149  [ 6400/60000]\n",
            "loss: 2.006855  [12800/60000]\n",
            "loss: 2.013236  [19200/60000]\n",
            "loss: 1.958607  [25600/60000]\n",
            "loss: 1.900748  [32000/60000]\n",
            "loss: 1.919373  [38400/60000]\n",
            "loss: 1.834610  [44800/60000]\n",
            "loss: 1.839853  [51200/60000]\n",
            "loss: 1.791060  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.3%, Avg loss: 1.791743 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.854845  [    0/60000]\n",
            "loss: 1.825025  [ 6400/60000]\n",
            "loss: 1.710732  [12800/60000]\n",
            "loss: 1.744631  [19200/60000]\n",
            "loss: 1.647276  [25600/60000]\n",
            "loss: 1.581121  [32000/60000]\n",
            "loss: 1.625606  [38400/60000]\n",
            "loss: 1.511760  [44800/60000]\n",
            "loss: 1.557809  [51200/60000]\n",
            "loss: 1.505556  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.9%, Avg loss: 1.518235 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.613616  [    0/60000]\n",
            "loss: 1.579435  [ 6400/60000]\n",
            "loss: 1.449772  [12800/60000]\n",
            "loss: 1.512185  [19200/60000]\n",
            "loss: 1.391231  [25600/60000]\n",
            "loss: 1.343859  [32000/60000]\n",
            "loss: 1.399325  [38400/60000]\n",
            "loss: 1.298408  [44800/60000]\n",
            "loss: 1.357077  [51200/60000]\n",
            "loss: 1.323909  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.6%, Avg loss: 1.336726 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.440807  [    0/60000]\n",
            "loss: 1.416320  [ 6400/60000]\n",
            "loss: 1.279746  [12800/60000]\n",
            "loss: 1.361866  [19200/60000]\n",
            "loss: 1.223394  [25600/60000]\n",
            "loss: 1.197326  [32000/60000]\n",
            "loss: 1.254903  [38400/60000]\n",
            "loss: 1.174353  [44800/60000]\n",
            "loss: 1.227735  [51200/60000]\n",
            "loss: 1.213734  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.3%, Avg loss: 1.218921 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.318470  [    0/60000]\n",
            "loss: 1.309300  [ 6400/60000]\n",
            "loss: 1.162048  [12800/60000]\n",
            "loss: 1.261397  [19200/60000]\n",
            "loss: 1.115811  [25600/60000]\n",
            "loss: 1.099594  [32000/60000]\n",
            "loss: 1.159888  [38400/60000]\n",
            "loss: 1.095672  [44800/60000]\n",
            "loss: 1.139021  [51200/60000]\n",
            "loss: 1.139588  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.2%, Avg loss: 1.137417 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.228050  [    0/60000]\n",
            "loss: 1.235512  [ 6400/60000]\n",
            "loss: 1.075705  [12800/60000]\n",
            "loss: 1.188433  [19200/60000]\n",
            "loss: 1.044083  [25600/60000]\n",
            "loss: 1.030277  [32000/60000]\n",
            "loss: 1.093769  [38400/60000]\n",
            "loss: 1.040126  [44800/60000]\n",
            "loss: 1.074301  [51200/60000]\n",
            "loss: 1.087271  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.3%, Avg loss: 1.078264 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.159205  [    0/60000]\n",
            "loss: 1.182660  [ 6400/60000]\n",
            "loss: 1.009972  [12800/60000]\n",
            "loss: 1.133243  [19200/60000]\n",
            "loss: 0.992811  [25600/60000]\n",
            "loss: 0.978576  [32000/60000]\n",
            "loss: 1.045955  [38400/60000]\n",
            "loss: 0.997504  [44800/60000]\n",
            "loss: 1.024868  [51200/60000]\n",
            "loss: 1.049478  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.2%, Avg loss: 1.033739 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.104703  [    0/60000]\n",
            "loss: 1.143016  [ 6400/60000]\n",
            "loss: 0.958791  [12800/60000]\n",
            "loss: 1.090494  [19200/60000]\n",
            "loss: 0.954590  [25600/60000]\n",
            "loss: 0.938522  [32000/60000]\n",
            "loss: 1.009551  [38400/60000]\n",
            "loss: 0.965364  [44800/60000]\n",
            "loss: 0.985806  [51200/60000]\n",
            "loss: 1.021619  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.1%, Avg loss: 0.999034 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiEF7C4Y38-Z"
      },
      "source": [
        "さらなる詳細\n",
        "--------------\n",
        "以下のページも参考ください。\n",
        "\n",
        "- [`Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "- [`torch.optim](https://pytorch.org/docs/stable/optim.html)\n",
        "- [`Warmstart Training a Model](https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrbzhvZTS2zq"
      },
      "source": [
        "以上。"
      ]
    }
  ]
}